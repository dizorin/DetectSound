{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAR2KCeZGVX3"
   },
   "source": [
    "# Applying Machine Learning on UrbanSound8k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USPAMetFGQ7O"
   },
   "source": [
    "## Install Packages\n",
    "\n",
    "We install: \n",
    "- Machine learning libraries: `tensorflow`, `sklearn`\n",
    "- Audio processing: `librosa`\n",
    "- Plots: `Plotly`, `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "sefSvCUtzKfx",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "23dc4b12-3f78-4f3c-a5e0-a06b2c5e2b3f",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/dizorin/.local/lib/python3.9/site-packages (1.4.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/dizorin/.local/lib/python3.9/site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/lib/python3.9/site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/dizorin/.local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.9/site-packages (53.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.9/site-packages (1.20.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/dizorin/.local/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/dizorin/.local/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/dizorin/.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/dizorin/.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.20.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dizorin/.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: librosa in /home/dizorin/.local/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.9/site-packages (from librosa) (20.9)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/lib/python3.9/site-packages (from librosa) (1.20.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/dizorin/.local/lib/python3.9/site-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/dizorin/.local/lib/python3.9/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/dizorin/.local/lib/python3.9/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: numba>=0.45.1 in /home/dizorin/.local/lib/python3.9/site-packages (from librosa) (0.55.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/dizorin/.local/lib/python3.9/site-packages (from librosa) (1.0.2)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /home/dizorin/.local/lib/python3.9/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/dizorin/.local/lib/python3.9/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/dizorin/.local/lib/python3.9/site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: audioread>=2.1.5 in /home/dizorin/.local/lib/python3.9/site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/dizorin/.local/lib/python3.9/site-packages (from numba>=0.45.1->librosa) (0.38.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.9/site-packages (from numba>=0.45.1->librosa) (53.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3.9/site-packages (from packaging>=20.0->librosa) (2.4.7)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/lib/python3.9/site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/lib/python3.9/site-packages (from pooch>=1.0->librosa) (2.25.1)\n",
      "Requirement already satisfied: six>=1.3 in /usr/lib/python3.9/site-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/dizorin/.local/lib/python3.9/site-packages (from scikit-learn>=0.19.1->librosa) (3.1.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/lib/python3.9/site-packages (from soundfile>=0.10.2->librosa) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/lib/python3.9/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3.9/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: plotly in /home/dizorin/.local/lib/python3.9/site-packages (5.7.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/dizorin/.local/lib/python3.9/site-packages (from plotly) (8.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3.9/site-packages (from plotly) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/dizorin/.local/lib/python3.9/site-packages (3.5.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/dizorin/.local/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/dizorin/.local/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3.9/site-packages (from matplotlib) (1.20.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/dizorin/.local/lib/python3.9/site-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/dizorin/.local/lib/python3.9/site-packages (from matplotlib) (4.31.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.9/site-packages (from matplotlib) (20.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3.9/site-packages (from matplotlib) (8.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3.9/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pillow in /usr/lib/python3.9/site-packages (8.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /home/dizorin/.local/lib/python3.9/site-packages (2.8.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.8.0-cp39-cp39-manylinux2010_x86_64.whl (497.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.6/497.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.6.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast>=0.2.1\n",
      "  Downloading gast-0.5.3-py3-none-any.whl (19 kB)\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
      "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m695.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/lib/python3.9/site-packages (from tensorflow) (3.12.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/lib/python3.9/site-packages (from tensorflow) (1.20.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3.9/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=9.0.1\n",
      "  Downloading libclang-13.0.0-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=0.4.0\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.7/126.7 KB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.24.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /home/dizorin/.local/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.9/site-packages (from tensorflow) (53.1.0)\n",
      "Collecting flatbuffers>=1.12\n",
      "  Downloading flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/dizorin/.local/lib/python3.9/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/dizorin/.local/lib/python3.9/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting keras-preprocessing>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m827.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: keras<2.9,>=2.8.0rc0 in /home/dizorin/.local/lib/python3.9/site-packages (from tensorflow) (2.8.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /home/dizorin/.local/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.36.2)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 KB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3.9/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.25.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.3-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.6/156.6 KB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.8-py3-none-any.whl (39 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: chardet>=3.0.2 in /usr/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna>=2.5 in /usr/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.3)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.1.0)\n",
      "Installing collected packages: tf-estimator-nightly, tensorboard-plugin-wit, libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorboard-data-server, rsa, opt-einsum, keras-preprocessing, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.3 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 h5py-3.6.0 keras-preprocessing-1.1.2 libclang-13.0.0 opt-einsum-3.3.0 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install setuptools\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install librosa\n",
    "!pip install plotly\n",
    "!pip install matplotlib\n",
    "!pip install pillow\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ikg2sdn36H0",
    "outputId": "9d5d94ed-526e-46d3-bd4b-9c56d14fb6bc"
   },
   "outputs": [],
   "source": [
    "# Unzip dataset\n",
    "import os\n",
    "\n",
    "if not os.path.isdir(\"UrbanSound8K\"):\n",
    "    !wget https://zenodo.org/record/1203745/files/UrbanSound8K.tar.gz -O urban8k.tgz\n",
    "    !tar -xzf urban8k.tgz\n",
    "    !rm urban8k.tgz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MrVjHhS40MZM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 22:27:38.595860: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-16 22:27:38.595936: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.8.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import librosa\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOx82BdSom6q"
   },
   "source": [
    "## Design Choices and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YbZ0ZqyosFi"
   },
   "source": [
    "Проанализировав набор данных и потратив немного времени на чтение о современном состоянии классификации аудиосигналов, я сделал следующие варианты дизайна и предложения:\n",
    "\n",
    "Обучите свёрточную нейронную сеть и используйте в качестве входных данных MFCC, STFT или Mel-Spectogram.\n",
    "\n",
    "- Поскольку продолжительность аудио варьируется от 0 до 4 с, я дополняю сгенерированную спектрограмму, чтобы сделать все аудио равной длины.\n",
    "\n",
    "Варианты признаков:\n",
    "\n",
    "Использование MFCC в качестве признаков:\n",
    "   - Обычно вычисляют первые 13 MFCC, их производные и вторые производные и используют их в качестве признаков.\n",
    "   - Или также обычно используется 40 MFCC, так как это значение по умолчанию для Librosa.\n",
    "\n",
    "- Использование STFT в качестве признаков:\n",
    "   - Содержит меньше человеческой обработки, чем MFCC и Mel-Spectogram, CNN может изучать другие фильтры, а не представления, разработанные людьми.\n",
    "\n",
    "- Использование Mel-Spectogram в качестве признаков:\n",
    "   - Преобразование, примененное к STFT, которое приблизительно соответствует тому, как люди воспринимают звук. Немного проще чем MFCC, но немного сложнее чем STFT.\n",
    "\n",
    "Моим первым выбором было бы использование STFT и Mel-Spectogram, поскольку похоже, что CNN могут использовать больше преимуществ частотно-временной структуры, но из-за **вычислительных ресурсов** и ограниченного времени я покажу использование **MFCC** в качестве функций. поскольку они намного более эффективны с точки зрения памяти.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctXVt_OEsrie"
   },
   "source": [
    "## Dataset Preprocessing and Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J728zt898gNW"
   },
   "source": [
    "Я загружаю все аудиоданные с помощью Librosa и частоты дискретизации по умолчанию 22050 Гц. Это дизайнерское решение основано на\n",
    "([Источник](https://librosa.org/blog/2019/07/17/resample-on-load/#Okay...-but-why-22050-Hz?--Why-not-44100-or-48000?)) и в дальнейших экспериментах можно было попробовать разные частоты дискретизации.\n",
    "\n",
    "> Люди могут слышать примерно до 20000 Гц, можно успешно анализировать музыкальные и речевые данные на гораздо более низких частотах без особых потерь. Самые высокие частоты, которые мы обычно стараемся обнаружить, составляют около C9 ≈ 8372 Гц, что значительно ниже порога 11025, подразумеваемого fs = 22050.\n",
    "\n",
    "По умолчанию Librosa загружает звук в моно, давая нам 1 канал.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "Bzfg9qYAH0NC"
   },
   "outputs": [],
   "source": [
    "# FeatureExtractor class including librosa audio processing functions\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, csv_file):\n",
    "        self.csv_file = csv_file\n",
    "        self.max_audio_duration = 4\n",
    "        self.dataset_df = self._create_dataset(csv_file)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_dataset(csv_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_path: path with the .wav files after unzipping\n",
    "        Returns: A pandas dataframe with the list of files and labels (`filenames`, `labels`)\n",
    "        \"\"\"\n",
    "        dataset_df = pd.read_csv(csv_file)\n",
    "        filepaths = []\n",
    "        for i, row in dataset_df.iterrows():\n",
    "            filepaths.append(os.path.join('UrbanSound8K/audio', 'fold'+str(row['fold']), row['slice_file_name']))\n",
    "        dataset_df['filepath'] = filepaths\n",
    "        return dataset_df\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_max_pad_length(max_audio_length, sample_rate=22050, n_fft=2048, hop_length=512):\n",
    "        dummy_file = np.random.random(max_audio_length*sample_rate)\n",
    "        stft = librosa.stft(dummy_file, n_fft=n_fft, hop_length=hop_length)\n",
    "        # Return an even number for CNN computation purposes\n",
    "        if stft.shape[1] % 2 != 0:\n",
    "            return stft.shape[1] + 1\n",
    "        return stft.shape[1]\n",
    "\n",
    "    def compute_save_features(self, \n",
    "                        mode='mfcc', \n",
    "                        sample_rate=22050,\n",
    "                        n_fft=2048,\n",
    "                        hop_length=512,\n",
    "                        n_mfcc=40,\n",
    "                        output_path='features',\n",
    "                        deltas=False\n",
    "                        ):\n",
    "        dataset_features = []\n",
    "        max_pad = self._compute_max_pad_length(self.max_audio_duration, \n",
    "                                               sample_rate=sample_rate, \n",
    "                                               n_fft=n_fft,\n",
    "                                               hop_length=hop_length)\n",
    "        print('Max Padding = ', max_pad)\n",
    "        \n",
    "        if not os.path.exists(output_path):\n",
    "            print('Creating output folder: ', output_path)\n",
    "            os.makedirs(output_path)\n",
    "        else:\n",
    "            print('Output folder already existed')\n",
    "            \n",
    "        print('Saving features in ', output_path)\n",
    "        i = 0\n",
    "        t = time.time()\n",
    "        \n",
    "        features_path = []\n",
    "        for filepath in self.dataset_df['filepath']:\n",
    "            if i % 100 == 0:\n",
    "                print('{} files processed in {}s'.format(i, time.time() - t))\n",
    "            audio_file, sample_rate = librosa.load(filepath, sr=sample_rate, res_type='kaiser_fast')\n",
    "            if mode == 'mfcc':\n",
    "                audio_features = self.compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas)  \n",
    "            elif mode == 'stft':\n",
    "                audio_features = self.compute_stft(audio_file, sample_rate, n_fft, hop_length)\n",
    "            elif mode == 'mel-spectogram':\n",
    "                audio_features = self.compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length)\n",
    "            \n",
    "            # print('audio_features',audio_features.shape)\n",
    "            audio_features = np.pad(audio_features, \n",
    "                                    pad_width=((0, 0), (0, max_pad - audio_features.shape[1]),(0, 0)))\n",
    "            \n",
    "            # print('audio_features pad',audio_features.shape)\n",
    "            \n",
    "            save_path = os.path.join(output_path, filepath.split('/')[-1].replace('wav', 'npy'))\n",
    "            self.save_features(audio_features, save_path)\n",
    "            features_path.append(save_path)\n",
    "            i+=1\n",
    "        self.dataset_df['features_path'] = features_path\n",
    "        return self.dataset_df\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_features(audio_features, filepath):\n",
    "        np.save(filepath, audio_features)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_mel_spectogram(audio_file, sample_rate, n_fft, hop_length):\n",
    "        return librosa.feature.melspectrogram(y=audio_file,\n",
    "                                              sr=sample_rate, \n",
    "                                              n_fft=n_fft,\n",
    "                                              hop_length=hop_length)\n",
    "    @staticmethod\n",
    "    def compute_stft(audio_file, sample_rate, n_fft, hop_length):\n",
    "        return librosa.stft(audio_file, n_fft=n_fft, hop_length=hop_length)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_mfcc(audio_file, sample_rate, n_fft, hop_length, n_mfcc, deltas=False):\n",
    "        mfccs = librosa.feature.mfcc(y=audio_file,\n",
    "                                    sr=sample_rate, \n",
    "                                    n_fft=n_fft,\n",
    "                                    n_mfcc=n_mfcc,\n",
    "                                    )\n",
    "        # Change mode from interpolation to nearest\n",
    "        if deltas:\n",
    "            mfccs = np.expand_dims(mfccs,axis=2)\n",
    "            \n",
    "            delta_mfccs = librosa.feature.delta(mfccs, mode='nearest')\n",
    "            delta2_mfccs = librosa.feature.delta(mfccs, order=2, mode='nearest')\n",
    "            \n",
    "            return np.concatenate((mfccs, delta_mfccs, delta2_mfccs),axis=2)\n",
    "        return mfccs\n",
    "\n",
    "# Create dataset and extract features\n",
    "fe = FeatureExtractor('UrbanSound8K/metadata/UrbanSound8K.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "Y9fa-rfjH9rn",
    "outputId": "8efaa040-3f34-40dc-cf16-b85a44eed3ee",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Padding =  174\n",
      "Creating output folder:  features_mfcc\n",
      "Saving features in  features_mfcc\n",
      "0 files processed in 0.00031113624572753906s\n",
      "100 files processed in 12.606899976730347s\n",
      "200 files processed in 24.748304843902588s\n",
      "300 files processed in 38.89931797981262s\n",
      "400 files processed in 51.65732264518738s\n",
      "500 files processed in 64.08351802825928s\n",
      "600 files processed in 76.73673844337463s\n",
      "700 files processed in 90.54266810417175s\n",
      "800 files processed in 106.96380186080933s\n",
      "900 files processed in 119.08151531219482s\n",
      "1000 files processed in 130.72327971458435s\n",
      "1100 files processed in 143.8911895751953s\n",
      "1200 files processed in 157.42307019233704s\n",
      "1300 files processed in 171.55621576309204s\n",
      "1400 files processed in 184.84662318229675s\n",
      "1500 files processed in 194.1484808921814s\n",
      "1600 files processed in 208.56130170822144s\n",
      "1700 files processed in 221.17717218399048s\n",
      "1800 files processed in 234.45928263664246s\n",
      "1900 files processed in 245.62158918380737s\n",
      "2000 files processed in 256.79094409942627s\n",
      "2100 files processed in 267.6115868091583s\n",
      "2200 files processed in 280.14271998405457s\n",
      "2300 files processed in 292.27882385253906s\n",
      "2400 files processed in 303.733562707901s\n",
      "2500 files processed in 314.98282527923584s\n",
      "2600 files processed in 325.92598009109497s\n",
      "2700 files processed in 338.12390303611755s\n",
      "2800 files processed in 351.0850820541382s\n",
      "2900 files processed in 366.79125571250916s\n",
      "3000 files processed in 382.0274021625519s\n",
      "3100 files processed in 392.5503776073456s\n",
      "3200 files processed in 403.51414227485657s\n",
      "3300 files processed in 415.4423449039459s\n",
      "3400 files processed in 427.5385272502899s\n",
      "3500 files processed in 439.7413082122803s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dizorin/.local/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1323\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600 files processed in 452.9502897262573s\n",
      "3700 files processed in 465.0124680995941s\n",
      "3800 files processed in 476.74525237083435s\n",
      "3900 files processed in 490.2759349346161s\n",
      "4000 files processed in 502.8731346130371s\n",
      "4100 files processed in 515.9406824111938s\n",
      "4200 files processed in 527.1015446186066s\n",
      "4300 files processed in 538.9200437068939s\n",
      "4400 files processed in 559.7939870357513s\n",
      "4500 files processed in 571.1860523223877s\n",
      "4600 files processed in 582.9600696563721s\n",
      "4700 files processed in 596.495890378952s\n",
      "4800 files processed in 609.6102838516235s\n",
      "4900 files processed in 623.4087707996368s\n",
      "5000 files processed in 639.0206425189972s\n",
      "5100 files processed in 653.365775346756s\n",
      "5200 files processed in 668.2914254665375s\n",
      "5300 files processed in 683.5758645534515s\n",
      "5400 files processed in 696.6333961486816s\n",
      "5500 files processed in 708.2717821598053s\n",
      "5600 files processed in 719.531896352768s\n",
      "5700 files processed in 730.679196357727s\n",
      "5800 files processed in 742.8220293521881s\n",
      "5900 files processed in 753.3603343963623s\n",
      "6000 files processed in 763.3726415634155s\n",
      "6100 files processed in 775.6859085559845s\n",
      "6200 files processed in 783.6898715496063s\n",
      "6300 files processed in 794.4835963249207s\n",
      "6400 files processed in 806.9364080429077s\n",
      "6500 files processed in 816.6560616493225s\n",
      "6600 files processed in 827.1254253387451s\n",
      "6700 files processed in 838.440160036087s\n",
      "6800 files processed in 851.2827661037445s\n",
      "6900 files processed in 863.7761991024017s\n",
      "7000 files processed in 876.1579024791718s\n",
      "7100 files processed in 888.5378000736237s\n",
      "7200 files processed in 901.2658867835999s\n",
      "7300 files processed in 913.3310573101044s\n",
      "7400 files processed in 922.7886018753052s\n",
      "7500 files processed in 935.98566198349s\n",
      "7600 files processed in 947.6667039394379s\n",
      "7700 files processed in 959.3820831775665s\n",
      "7800 files processed in 972.17338347435s\n",
      "7900 files processed in 985.0715634822845s\n",
      "8000 files processed in 996.8279752731323s\n",
      "8100 files processed in 1007.8589861392975s\n",
      "8200 files processed in 1019.8748350143433s\n",
      "8300 files processed in 1030.7023227214813s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dizorin/.local/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1103\n",
      "  return f(*args, **kwargs)\n",
      "/home/dizorin/.local/lib/python3.9/site-packages/librosa/util/decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1523\n",
      "  return f(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8400 files processed in 1041.194732427597s\n",
      "8500 files processed in 1053.2792451381683s\n",
      "8600 files processed in 1065.676104068756s\n",
      "8700 files processed in 1076.688779592514s\n"
     ]
    }
   ],
   "source": [
    "#run to compute and save features on the colab notebook\n",
    "if not os.path.isdir(\"features_mfcc\"):\n",
    "    dataset_df = fe.compute_save_features(mode='mfcc', n_mfcc=13, output_path='features_mfcc', deltas=True)\n",
    "    dataset_df.to_json('dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4wZmY8n8od-"
   },
   "source": [
    "Доступ к диску и librosa загрузка аудиофайлов очень медленная на ноутбуке colab (30-40 минут), вместо этого мы могли бы загрузить предварительно вычисленные функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9x6AFuuuQvb",
    "outputId": "d058dfda-22cc-42e1-f573-3e832bf9fa2f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Unzip features\n",
    "if not os.path.isdir(\"features_mfcc\"):\n",
    "    !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1BU2B5EcbfyGBIOkB5YC44hpzPpuqw43H\" -O features_mfcc.zip && rm -rf /tmp/cookies.txt\n",
    "    !unzip -q features_mfcc.zip\n",
    "    !rm features_mfcc.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VI_RjDQLBmpb",
    "outputId": "1cd49fbe-78a2-49f2-cd47-4728951e5650",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download dataset.json file\n",
    "if not os.path.isfile(\"dataset.json\"):\n",
    "    !wget --no-check-certificate \"https://docs.google.com/uc?export=download&id=1pzSvGYaBXghLQFTZxlSex-Ts3T4B0X4C\" -O dataset.json\n",
    "\n",
    "dataset_df = pd.read_json('dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giVHke5JwwYh"
   },
   "source": [
    "Для целей этого эксперимента мы будем загружать все данные в память и обрабатывать их в мини-пакетах. Если бы у нас были вычислительные ресурсы и больше времени, мы могли бы создавать объекты Dataloader, которые позволяли бы выполнять многие другие операции, такие как увеличение данных, и выполнять итерации быстрее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "id": "HfDBipkQz1RF",
    "outputId": "2f9b81b2-a4d0-4e58-b62a-d90aed9c982f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "      <th>filepath</th>\n",
       "      <th>features_path</th>\n",
       "      <th>features</th>\n",
       "      <th>labels_categorical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "      <td>UrbanSound8K/audio/fold5/100032-3-0-0.wav</td>\n",
       "      <td>features_mfcc/100032-3-0-0.npy</td>\n",
       "      <td>[[[-336.0325, 1.1658469e-15, -7.1054274e-15], ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>UrbanSound8K/audio/fold5/100263-2-0-117.wav</td>\n",
       "      <td>features_mfcc/100263-2-0-117.npy</td>\n",
       "      <td>[[[-492.85135, 1.7099216e-15, -7.1054274e-15],...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>UrbanSound8K/audio/fold5/100263-2-0-121.wav</td>\n",
       "      <td>features_mfcc/100263-2-0-121.npy</td>\n",
       "      <td>[[[-497.33685, 1.7254838e-15, -1.4210855e-14],...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>UrbanSound8K/audio/fold5/100263-2-0-126.wav</td>\n",
       "      <td>features_mfcc/100263-2-0-126.npy</td>\n",
       "      <td>[[[-448.51984, 1.5561158e-15, -7.1054274e-15],...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "      <td>UrbanSound8K/audio/fold5/100263-2-0-137.wav</td>\n",
       "      <td>features_mfcc/100263-2-0-137.npy</td>\n",
       "      <td>[[[-474.92578, 1.6477298e-15, -7.1054274e-15],...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class                                     filepath  \\\n",
       "0          dog_bark    UrbanSound8K/audio/fold5/100032-3-0-0.wav   \n",
       "1  children_playing  UrbanSound8K/audio/fold5/100263-2-0-117.wav   \n",
       "2  children_playing  UrbanSound8K/audio/fold5/100263-2-0-121.wav   \n",
       "3  children_playing  UrbanSound8K/audio/fold5/100263-2-0-126.wav   \n",
       "4  children_playing  UrbanSound8K/audio/fold5/100263-2-0-137.wav   \n",
       "\n",
       "                      features_path  \\\n",
       "0    features_mfcc/100032-3-0-0.npy   \n",
       "1  features_mfcc/100263-2-0-117.npy   \n",
       "2  features_mfcc/100263-2-0-121.npy   \n",
       "3  features_mfcc/100263-2-0-126.npy   \n",
       "4  features_mfcc/100263-2-0-137.npy   \n",
       "\n",
       "                                            features  \\\n",
       "0  [[[-336.0325, 1.1658469e-15, -7.1054274e-15], ...   \n",
       "1  [[[-492.85135, 1.7099216e-15, -7.1054274e-15],...   \n",
       "2  [[[-497.33685, 1.7254838e-15, -1.4210855e-14],...   \n",
       "3  [[[-448.51984, 1.5561158e-15, -7.1054274e-15],...   \n",
       "4  [[[-474.92578, 1.6477298e-15, -7.1054274e-15],...   \n",
       "\n",
       "                                  labels_categorical  \n",
       "0  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df['features'] = [np.asarray(np.load(feature_path)) for feature_path in dataset_df['features_path']]\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "dataset_df['labels_categorical'] = [to_categorical(label, 10) for label in dataset_df['classID']]\n",
    "\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3pQN4JBIxWeP"
   },
   "source": [
    "Мы собираемся создать разделения для обучающих, проверочных и тестовых наборов данных.\n",
    "В целях эксперимента и для его ускорения мы будем использовать функцию sklearn `train_test_split` два раза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-k-OKxXqkwh",
    "outputId": "94e51f18-49f1-47d3-e6a3-a0fef714d692"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-16 22:28:07.662862: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-04-16 22:28:07.662945: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ArchPC\n",
      "2022-04-16 22:28:07.662961: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ArchPC\n",
      "2022-04-16 22:28:07.663152: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 390.141.0\n",
      "2022-04-16 22:28:07.663218: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.141.0\n",
      "2022-04-16 22:28:07.663229: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 390.141.0\n",
      "2022-04-16 22:28:07.664550: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-16 22:28:07.684131: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 165904128 exceeds 10% of free system memory.\n",
      "2022-04-16 22:28:08.944604: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 165904128 exceeds 10% of free system memory.\n",
      "2022-04-16 22:28:09.085437: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 165904128 exceeds 10% of free system memory.\n",
      "2022-04-16 22:28:09.253343: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 165904128 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6112, 13, 174, 3) (1310, 13, 174, 3) (1310, 13, 174, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Add one dimension for the channel\n",
    "X = np.array(dataset_df['features'].tolist())\n",
    "y = np.array(dataset_df['labels_categorical'].tolist())\n",
    "\n",
    "# As there is unbalance for some classes I am going to stratify it so we have the same proportion in train/test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=1, \n",
    "                                                    stratify=y)\n",
    "# Create validation and test\n",
    "X_test, X_val, Y_test, Y_val = train_test_split(X_test, \n",
    "                                                Y_test, \n",
    "                                                test_size=0.5, \n",
    "                                                random_state=1, \n",
    "                                                stratify=Y_test)\n",
    "\n",
    "norm1 = tf.keras.layers.Normalization()\n",
    "norm1.adapt(X_train)\n",
    "\n",
    "X_train = norm1(X_train)\n",
    "X_val = norm1(X_val)\n",
    "X_test = norm1(X_test)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mbA_78g6iUa"
   },
   "source": [
    "## Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UXoQEZ16pjr"
   },
   "source": [
    "### Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-sygDIE29Og"
   },
   "source": [
    "Мы собираемся создать модель **Fully Convolutional Network**, используя Keras, работающий поверх Tensorflow с несколькими слоями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ko1n0Tla3n1y"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, Conv2D, MaxPooling2D, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y589P2Qm_AuG"
   },
   "source": [
    "Поскольку наши изображения имеют прямоугольную форму (ось y — это MFCC, ось x — время), вместо использования квадратных фильтров (как обычно) мы собираемся сделать их прямоугольными, чтобы они могли лучше изучить корреляцию MFCC с временным измерением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FdM5w8r1kMgs"
   },
   "outputs": [],
   "source": [
    "# FCN Model\n",
    "def create_model(num_classes=10, model_type=None, input_shape=None, dropout_ratio=None):\n",
    "    model = Sequential()\n",
    "    if input_shape is None:\n",
    "        model.add(Input(shape=(None, None, 1)))\n",
    "    else:\n",
    "        model.add(Input(shape=input_shape))\n",
    "        \n",
    "    model.add(tf.keras.layers.RandomContrast(factor=(0.2,1.8))) #(x - mean) * factor + mean \n",
    "    model.add(tf.keras.layers.RandomTranslation(height_factor=0, width_factor=(-0.2, 0.2),fill_mode='constant'))\n",
    "    model.add(tf.keras.layers.RandomZoom(height_factor=0, width_factor=(-0.2, 0.2),fill_mode='constant'))\n",
    "    \n",
    "    if model_type is None:\n",
    "        model.add(Conv2D(filters=16, kernel_size=(2, 4), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 3)))\n",
    "        model.add(Conv2D(filters=32, kernel_size=(2, 4), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(Conv2D(filters=64, kernel_size=(2, 4), activation='relu'))\n",
    "        model.add(MaxPooling2D(pool_size=2))\n",
    "        model.add(Conv2D(filters=128, kernel_size=(2, 4), activation='relu'))\n",
    "    else:\n",
    "        #model.add(Conv2D(3,1))\n",
    "        \n",
    "        if model_type == 'VGG16':\n",
    "            vgg = tf.keras.applications.VGG16(weights='imagenet', include_top=False)\n",
    "            model.add(tf.keras.Model(inputs=vgg.get_layer(\"block1_conv1\").input,\n",
    "                                outputs=vgg.get_layer('block5_conv3').output))\n",
    "            \n",
    "        elif model_type == 'EfficientNetV2B1':\n",
    "            eff = tf.keras.applications.EfficientNetV2B1(weights=None, include_top=False, classes=100)\n",
    "            model.add(tf.keras.Model(inputs=eff.input,\n",
    "                            outputs=eff.get_layer('top_activation').output))\n",
    "            \n",
    "        elif model_type == 'EfficientNetV2B0':\n",
    "            eff0 = tf.keras.applications.EfficientNetV2B0(weights=None, include_top=False, classes=100)\n",
    "            # eff0.summary()\n",
    "            model.add(tf.keras.Model(inputs=eff0.input,\n",
    "                            outputs=eff0.get_layer('top_activation').output))\n",
    "            \n",
    "        elif model_type == 'MobileNetV2':\n",
    "            mb = tf.keras.applications.MobileNetV2(weights=None, include_top=False, classes=100)\n",
    "            # mb.summary()\n",
    "            model.add(tf.keras.Model(inputs=mb.input,\n",
    "                            outputs=mb.output))\n",
    "            \n",
    "    model.add(Flatten())\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    # model.add(GlobalAveragePooling2D())\n",
    "    # # model.add(Flatten())\n",
    "    # if dropout_ratio is not None:\n",
    "    #     model.add(Dropout(dropout_ratio))\n",
    "    # Add dense linear layer\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj73VymN-ueo"
   },
   "source": [
    "Поскольку это проблема множественной классификации, мы будем использовать **Categorical Cross Entropy loss**. В качестве оптимизатора мы будем использовать реализацию Keras **Adam** со значениями гиперпараметров по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vaj8BrhD4cyH",
    "outputId": "1270aa80-24fb-43a0-cb72-4fd607c2ff86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " random_contrast_1 (RandomCo  (None, 13, 174, 3)       0         \n",
      " ntrast)                                                         \n",
      "                                                                 \n",
      " random_translation_1 (Rando  (None, 13, 174, 3)       0         \n",
      " mTranslation)                                                   \n",
      "                                                                 \n",
      " random_zoom_1 (RandomZoom)  (None, 13, 174, 3)        0         \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, None, None, 1280)  5919312   \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 7680)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               983168    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,911,386\n",
      "Trainable params: 6,850,778\n",
      "Non-trainable params: 60,608\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create and compile the model\n",
    "fcn_model = create_model(input_shape=X_train.shape[1:], model_type='EfficientNetV2B0')\n",
    "fcn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "fcn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWTJXf4B6uNW"
   },
   "source": [
    "### Model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lBxKnyeo64TG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qTT0_wO27AFy",
    "outputId": "0ac0fbdd-ee31-46d1-fc99-751db2e25c2e"
   },
   "outputs": [],
   "source": [
    "!mkdir saved_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "N1hJEl2f5JLI"
   },
   "outputs": [],
   "source": [
    "def train_model(model, X_train, Y_train, X_val, Y_val, epochs, batch_size, callbacks):\n",
    "    history = model.fit(X_train, \n",
    "              Y_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              validation_data=(X_val, Y_val), \n",
    "              callbacks=callbacks, verbose=1)\n",
    "    \n",
    "    fig, (ax0, ax1) = plt.subplots(1,2, figsize=(10,5))\n",
    "\n",
    "    # print(list(history.history))\n",
    "    ax0.plot(history.history['accuracy'])\n",
    "    ax0.plot(history.history['val_accuracy'])\n",
    "    ax0.set_title('model accuracy')\n",
    "    ax0.set_ylabel('accuracy')\n",
    "    ax0.set_xlabel('epoch')\n",
    "    ax0.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "    ax1.set_title('model loss')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfzcdmYtETt3"
   },
   "source": [
    "Мы создадим контрольную точку для **ранней остановки**, поэтому мы выберем модель, которая лучше работает на проверочном наборе.\n",
    "\n",
    "Создание функции для обучения модели позволит нам быстрее выполнять настройку гиперпараметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Nt9Q7KQ67WmV"
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/best_fcn_eff2B0.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True)\n",
    "callbacks = [checkpointer]\n",
    "\n",
    "# fcn_model = load_model('saved_models/best_fcn.hdf5')\n",
    "# fcn_model.summary()\n",
    "\n",
    "# Hyper-parameters\n",
    "epochs = 200\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5IB-m4c17E7e",
    "outputId": "c86afb1a-0c3f-4154-9d54-79ff29287690",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.4450 - accuracy: 0.1556\n",
      "Epoch 1: val_accuracy improved from -inf to 0.10611, saving model to saved_models/best_fcn_eff2B0.hdf5\n",
      "24/24 [==============================] - 115s 4s/step - loss: 2.4450 - accuracy: 0.1556 - val_loss: 2.3026 - val_accuracy: 0.1061\n",
      "Epoch 2/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.2123 - accuracy: 0.1808\n",
      "Epoch 2: val_accuracy did not improve from 0.10611\n",
      "24/24 [==============================] - 97s 4s/step - loss: 2.2123 - accuracy: 0.1808 - val_loss: 2.3477 - val_accuracy: 0.0840\n",
      "Epoch 3/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.1366 - accuracy: 0.2063\n",
      "Epoch 3: val_accuracy did not improve from 0.10611\n",
      "24/24 [==============================] - 96s 4s/step - loss: 2.1366 - accuracy: 0.2063 - val_loss: 2.6880 - val_accuracy: 0.0557\n",
      "Epoch 4/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.1032 - accuracy: 0.2246\n",
      "Epoch 4: val_accuracy did not improve from 0.10611\n",
      "24/24 [==============================] - 96s 4s/step - loss: 2.1032 - accuracy: 0.2246 - val_loss: 2.5615 - val_accuracy: 0.0954\n",
      "Epoch 5/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 2.0486 - accuracy: 0.2310\n",
      "Epoch 5: val_accuracy improved from 0.10611 to 0.14046, saving model to saved_models/best_fcn_eff2B0.hdf5\n",
      "24/24 [==============================] - 97s 4s/step - loss: 2.0486 - accuracy: 0.2310 - val_loss: 2.4580 - val_accuracy: 0.1405\n",
      "Epoch 6/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.9746 - accuracy: 0.2552\n",
      "Epoch 6: val_accuracy did not improve from 0.14046\n",
      "24/24 [==============================] - 96s 4s/step - loss: 1.9746 - accuracy: 0.2552 - val_loss: 2.2727 - val_accuracy: 0.1359\n",
      "Epoch 7/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.9066 - accuracy: 0.2772\n",
      "Epoch 7: val_accuracy did not improve from 0.14046\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.9066 - accuracy: 0.2772 - val_loss: 2.8495 - val_accuracy: 0.1214\n",
      "Epoch 8/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.8632 - accuracy: 0.2981\n",
      "Epoch 8: val_accuracy improved from 0.14046 to 0.27252, saving model to saved_models/best_fcn_eff2B0.hdf5\n",
      "24/24 [==============================] - 97s 4s/step - loss: 1.8632 - accuracy: 0.2981 - val_loss: 2.0577 - val_accuracy: 0.2725\n",
      "Epoch 9/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.8052 - accuracy: 0.3215\n",
      "Epoch 9: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 96s 4s/step - loss: 1.8052 - accuracy: 0.3215 - val_loss: 2.1444 - val_accuracy: 0.1748\n",
      "Epoch 10/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.7758 - accuracy: 0.3436\n",
      "Epoch 10: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 96s 4s/step - loss: 1.7758 - accuracy: 0.3436 - val_loss: 2.3194 - val_accuracy: 0.1053\n",
      "Epoch 11/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.6863 - accuracy: 0.3788\n",
      "Epoch 11: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.6863 - accuracy: 0.3788 - val_loss: 2.3072 - val_accuracy: 0.0893\n",
      "Epoch 12/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.6445 - accuracy: 0.3986\n",
      "Epoch 12: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 96s 4s/step - loss: 1.6445 - accuracy: 0.3986 - val_loss: 2.5981 - val_accuracy: 0.0855\n",
      "Epoch 13/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.5967 - accuracy: 0.4185\n",
      "Epoch 13: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.5967 - accuracy: 0.4185 - val_loss: 5.5197 - val_accuracy: 0.1481\n",
      "Epoch 14/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.5314 - accuracy: 0.4504\n",
      "Epoch 14: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.5314 - accuracy: 0.4504 - val_loss: 5.3018 - val_accuracy: 0.1145\n",
      "Epoch 15/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.4596 - accuracy: 0.4777\n",
      "Epoch 15: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 96s 4s/step - loss: 1.4596 - accuracy: 0.4777 - val_loss: 2.2881 - val_accuracy: 0.1145\n",
      "Epoch 16/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3830 - accuracy: 0.5070\n",
      "Epoch 16: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.3830 - accuracy: 0.5070 - val_loss: 2.2869 - val_accuracy: 0.1145\n",
      "Epoch 17/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3799 - accuracy: 0.5229\n",
      "Epoch 17: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.3799 - accuracy: 0.5229 - val_loss: 4.3801 - val_accuracy: 0.1229\n",
      "Epoch 18/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3254 - accuracy: 0.5425\n",
      "Epoch 18: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.3254 - accuracy: 0.5425 - val_loss: 3.0769 - val_accuracy: 0.0992\n",
      "Epoch 19/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3143 - accuracy: 0.5479\n",
      "Epoch 19: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.3143 - accuracy: 0.5479 - val_loss: 3.2641 - val_accuracy: 0.0817\n",
      "Epoch 20/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3162 - accuracy: 0.5481\n",
      "Epoch 20: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.3162 - accuracy: 0.5481 - val_loss: 10.7461 - val_accuracy: 0.1145\n",
      "Epoch 21/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.3595 - accuracy: 0.5358\n",
      "Epoch 21: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 96s 4s/step - loss: 1.3595 - accuracy: 0.5358 - val_loss: 3.3224 - val_accuracy: 0.1145\n",
      "Epoch 22/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.2887 - accuracy: 0.5566\n",
      "Epoch 22: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 96s 4s/step - loss: 1.2887 - accuracy: 0.5566 - val_loss: 2.2928 - val_accuracy: 0.1145\n",
      "Epoch 23/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.2236 - accuracy: 0.5794\n",
      "Epoch 23: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.2236 - accuracy: 0.5794 - val_loss: 3.2371 - val_accuracy: 0.1076\n",
      "Epoch 24/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.1677 - accuracy: 0.6062\n",
      "Epoch 24: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.1677 - accuracy: 0.6062 - val_loss: 7.2347 - val_accuracy: 0.0878\n",
      "Epoch 25/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.1116 - accuracy: 0.6283\n",
      "Epoch 25: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.1116 - accuracy: 0.6283 - val_loss: 4.1060 - val_accuracy: 0.0878\n",
      "Epoch 26/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.1250 - accuracy: 0.6279\n",
      "Epoch 26: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.1250 - accuracy: 0.6279 - val_loss: 4.2495 - val_accuracy: 0.1153\n",
      "Epoch 27/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.1098 - accuracy: 0.6245\n",
      "Epoch 27: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.1098 - accuracy: 0.6245 - val_loss: 3.7205 - val_accuracy: 0.2046\n",
      "Epoch 28/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.0949 - accuracy: 0.6386\n",
      "Epoch 28: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.0949 - accuracy: 0.6386 - val_loss: 6.7720 - val_accuracy: 0.1137\n",
      "Epoch 29/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.0613 - accuracy: 0.6455\n",
      "Epoch 29: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.0613 - accuracy: 0.6455 - val_loss: 5.2535 - val_accuracy: 0.1344\n",
      "Epoch 30/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.0763 - accuracy: 0.6394\n",
      "Epoch 30: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.0763 - accuracy: 0.6394 - val_loss: 5.3937 - val_accuracy: 0.1145\n",
      "Epoch 31/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 1.0190 - accuracy: 0.6656\n",
      "Epoch 31: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 1.0190 - accuracy: 0.6656 - val_loss: 4.4175 - val_accuracy: 0.1145\n",
      "Epoch 32/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.9754 - accuracy: 0.6819\n",
      "Epoch 32: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.9754 - accuracy: 0.6819 - val_loss: 2.4007 - val_accuracy: 0.1534\n",
      "Epoch 33/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.9770 - accuracy: 0.6775\n",
      "Epoch 33: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.9770 - accuracy: 0.6775 - val_loss: 39.9421 - val_accuracy: 0.1145\n",
      "Epoch 34/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.9848 - accuracy: 0.6872\n",
      "Epoch 34: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.9848 - accuracy: 0.6872 - val_loss: 4.6364 - val_accuracy: 0.1481\n",
      "Epoch 35/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.9674 - accuracy: 0.6868\n",
      "Epoch 35: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 97s 4s/step - loss: 0.9674 - accuracy: 0.6868 - val_loss: 2.3038 - val_accuracy: 0.1145\n",
      "Epoch 36/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.9019 - accuracy: 0.7098\n",
      "Epoch 36: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.9019 - accuracy: 0.7098 - val_loss: 6.4601 - val_accuracy: 0.1748\n",
      "Epoch 37/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8014 - accuracy: 0.7436\n",
      "Epoch 37: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.8014 - accuracy: 0.7436 - val_loss: 3.3480 - val_accuracy: 0.1580\n",
      "Epoch 38/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8245 - accuracy: 0.7392\n",
      "Epoch 38: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.8245 - accuracy: 0.7392 - val_loss: 3.7452 - val_accuracy: 0.1237\n",
      "Epoch 39/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8836 - accuracy: 0.7227\n",
      "Epoch 39: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.8836 - accuracy: 0.7227 - val_loss: 16.3475 - val_accuracy: 0.0725\n",
      "Epoch 40/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8334 - accuracy: 0.7320\n",
      "Epoch 40: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 96s 4s/step - loss: 0.8334 - accuracy: 0.7320 - val_loss: 6.5621 - val_accuracy: 0.1084\n",
      "Epoch 41/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7522 - accuracy: 0.7574\n",
      "Epoch 41: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.7522 - accuracy: 0.7574 - val_loss: 3.5582 - val_accuracy: 0.1466\n",
      "Epoch 42/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8962 - accuracy: 0.7153\n",
      "Epoch 42: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.8962 - accuracy: 0.7153 - val_loss: 3.0694 - val_accuracy: 0.1145\n",
      "Epoch 43/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.8617 - accuracy: 0.7259\n",
      "Epoch 43: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.8617 - accuracy: 0.7259 - val_loss: 2.2711 - val_accuracy: 0.1817\n",
      "Epoch 44/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7665 - accuracy: 0.7511\n",
      "Epoch 44: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.7665 - accuracy: 0.7511 - val_loss: 4.6474 - val_accuracy: 0.1328\n",
      "Epoch 45/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.7179 - accuracy: 0.7706\n",
      "Epoch 45: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.7179 - accuracy: 0.7706 - val_loss: 2.7009 - val_accuracy: 0.1702\n",
      "Epoch 46/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6413 - accuracy: 0.7986\n",
      "Epoch 46: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 96s 4s/step - loss: 0.6413 - accuracy: 0.7986 - val_loss: 5.8464 - val_accuracy: 0.1634\n",
      "Epoch 47/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6801 - accuracy: 0.7827\n",
      "Epoch 47: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.6801 - accuracy: 0.7827 - val_loss: 12.7840 - val_accuracy: 0.1397\n",
      "Epoch 48/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6409 - accuracy: 0.7934\n",
      "Epoch 48: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.6409 - accuracy: 0.7934 - val_loss: 6.4568 - val_accuracy: 0.1649\n",
      "Epoch 49/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6141 - accuracy: 0.8087\n",
      "Epoch 49: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.6141 - accuracy: 0.8087 - val_loss: 102.8657 - val_accuracy: 0.1145\n",
      "Epoch 50/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6070 - accuracy: 0.8094\n",
      "Epoch 50: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.6070 - accuracy: 0.8094 - val_loss: 3.2669 - val_accuracy: 0.1328\n",
      "Epoch 51/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.6044 - accuracy: 0.8056\n",
      "Epoch 51: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.6044 - accuracy: 0.8056 - val_loss: 4.9977 - val_accuracy: 0.1527\n",
      "Epoch 52/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5664 - accuracy: 0.8174\n",
      "Epoch 52: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.5664 - accuracy: 0.8174 - val_loss: 8.4039 - val_accuracy: 0.1748\n",
      "Epoch 53/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5619 - accuracy: 0.8279\n",
      "Epoch 53: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 96s 4s/step - loss: 0.5619 - accuracy: 0.8279 - val_loss: 28.4525 - val_accuracy: 0.1435\n",
      "Epoch 54/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5520 - accuracy: 0.8215\n",
      "Epoch 54: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.5520 - accuracy: 0.8215 - val_loss: 8.5085 - val_accuracy: 0.1221\n",
      "Epoch 55/200\n",
      "24/24 [==============================] - ETA: 0s - loss: 0.5547 - accuracy: 0.8258\n",
      "Epoch 55: val_accuracy did not improve from 0.27252\n",
      "24/24 [==============================] - 95s 4s/step - loss: 0.5547 - accuracy: 0.8258 - val_loss: 8.4459 - val_accuracy: 0.1145\n",
      "Epoch 56/200\n",
      "18/24 [=====================>........] - ETA: 23s - loss: 0.5922 - accuracy: 0.8212"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfcn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mY_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mY_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, X_train, Y_train, X_val, Y_val, epochs, batch_size, callbacks)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_model\u001b[39m(model, X_train, Y_train, X_val, Y_val, epochs, batch_size, callbacks):\n\u001b[0;32m----> 2\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m              \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m              \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m              \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     fig, (ax0, ax1) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# print(list(history.history))\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = train_model(model=fcn_model,\n",
    "                    X_train=X_train,\n",
    "                    X_val=X_val,\n",
    "                    Y_train=Y_train,\n",
    "                    Y_val=Y_val,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5a-BhlFDGj1",
    "outputId": "dfa6c322-9590-47ed-aa20-c55e7e11d22d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9874018430709839\n",
      "Validation Accuracy:  0.9297710061073303\n",
      "Testing Accuracy:  0.9137404561042786\n"
     ]
    }
   ],
   "source": [
    "# Load the best model\n",
    "best_model = load_model('saved_models/best_fcn_none.hdf5')\n",
    "\n",
    "# Evaluating the model on the training and testing set\n",
    "score = best_model.evaluate(X_train, Y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = best_model.evaluate(X_val, Y_val, verbose=0)\n",
    "print(\"Validation Accuracy: \", score[1])\n",
    "\n",
    "score = best_model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvkVp34YEwRG",
    "tags": []
   },
   "source": [
    "Похоже, что модель переоснащена тренировочными данными к концу обучения. Мы выбрали модель, которая показала лучшие результаты на проверочном наборе, сохраненном контрольной точкой. Сходство между проверкой и тестовой оценкой говорит нам о том, что наша методология обучения верна и что наш проверочный набор является хорошей оценкой производительности тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inagrMP3A4rB",
    "outputId": "cae75b96-e76c-4746-852b-a70747708bc6"
   },
   "outputs": [],
   "source": [
    "# Plot a confusion matrix\n",
    "from sklearn import metrics\n",
    "Y_pred = best_model.predict(X_test)\n",
    "matrix = metrics.confusion_matrix(Y_test.argmax(axis=1), Y_pred.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzTmmTg6A4oy"
   },
   "outputs": [],
   "source": [
    "# Confusion matrix code (from https://github.com/triagemd/keras-eval/blob/master/keras_eval/visualizer.py)\n",
    "def plot_confusion_matrix(cm, concepts, normalize=False, show_text=True, fontsize=18, figsize=(16, 12),\n",
    "                          cmap=plt.cm.coolwarm_r, save_path=None, show_labels=True):\n",
    "    '''\n",
    "    Plot confusion matrix provided in 'cm'\n",
    "    Args:\n",
    "        cm: Confusion Matrix, square sized numpy array\n",
    "        concepts: Name of the categories to show\n",
    "        normalize: If True, normalize values between 0 and ones. Not valid if negative values.\n",
    "        show_text: If True, display cell values as text. Otherwise only display cell colors.\n",
    "        fontsize: Size of text\n",
    "        figsize: Size of figure\n",
    "        cmap: Color choice\n",
    "        save_path: If `save_path` specified, save confusion matrix in that location\n",
    "    Returns: Nothing. Plots confusion matrix\n",
    "    '''\n",
    "\n",
    "    if cm.ndim != 2 or cm.shape[0] != cm.shape[1]:\n",
    "        raise ValueError('Invalid confusion matrix shape, it should be square and ndim=2')\n",
    "\n",
    "    if cm.shape[0] != len(concepts) or cm.shape[1] != len(concepts):\n",
    "        raise ValueError('Number of concepts (%i) and dimensions of confusion matrix do not coincide (%i, %i)' %\n",
    "                         (len(concepts), cm.shape[0], cm.shape[1]))\n",
    "\n",
    "    plt.rcParams.update({'font.size': fontsize})\n",
    "\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm_normalized\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(cm, vmin=np.min(cm), vmax=np.max(cm), alpha=0.8, cmap=cmap)\n",
    "\n",
    "    fig.colorbar(cax)\n",
    "    ax.xaxis.tick_bottom()\n",
    "    plt.ylabel('True label', fontweight='bold')\n",
    "    plt.xlabel('Predicted label', fontweight='bold')\n",
    "\n",
    "    if show_labels:\n",
    "        n_labels = len(concepts)\n",
    "        ax.set_xticklabels(concepts)\n",
    "        ax.set_yticklabels(concepts)\n",
    "        plt.xticks(np.arange(0, n_labels, 1.0), rotation='vertical')\n",
    "        plt.yticks(np.arange(0, n_labels, 1.0))\n",
    "    else:\n",
    "        plt.axis('off')\n",
    "\n",
    "    if show_text:\n",
    "        # http://stackoverflow.com/questions/21712047/matplotlib-imshow-matshow-display-values-on-plot\n",
    "        min_val, max_val = 0, len(concepts)\n",
    "        ind_array = np.arange(min_val, max_val, 1.0)\n",
    "        x, y = np.meshgrid(ind_array, ind_array)\n",
    "        for i, (x_val, y_val) in enumerate(zip(x.flatten(), y.flatten())):\n",
    "            c = cm[int(x_val), int(y_val)]\n",
    "            ax.text(y_val, x_val, c, va='center', ha='center')\n",
    "\n",
    "    if save_path is not None:\n",
    "        plt.savefig(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX4fa42o1zyW"
   },
   "source": [
    "Чтобы лучше наблюдать за производительностью модели и ошибками, допущенными между разными классами, мы строим матрицу путаницы.\n",
    "\n",
    "В нашем случае точность является хорошей метрикой, потому что набор данных в основном сбалансирован, но мы наблюдали несколько классов с меньшим количеством выборок (1 «car_horn», «gun_shot» и «siren»), поэтому будет хорошо наблюдать за производительностью этих классов. .\n",
    "\n",
    "Мы можем заметить, что между классом Children_playing и классом street_music происходит много ошибок, поэтому, возможно, стоит потратить немного больше времени на анализ и поиск возможных причин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WabP5FBC8ph"
   },
   "outputs": [],
   "source": [
    "class_dictionary = {3: 'dog_bark', 2: 'children_playing', 1: 'car_horn', 0: 'air_conditioner', 9: 'street_music', 6: 'gun_shot', 8: 'siren', 5: 'engine_idling', 7: 'jackhammer', 4: 'drilling'}\n",
    "classes = [class_dictionary[key] for key in sorted(class_dictionary.keys())]\n",
    "\n",
    "plot_confusion_matrix(matrix, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12pnVV0xLiBr"
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "Мы можем наблюдать увеличение точности набора тестов на 1-2% при введении отсева в качестве регуляризации. Это показывает, что это было успешное дополнение к нашей модели.\n",
    "\n",
    "Есть много вещей, которые мы можем попробовать улучшить производительность модели, например:\n",
    "\n",
    "- Настройка гиперпараметров:\n",
    "   - Настройка параметров извлечения признаков\n",
    "   - Настройка параметров сети (количество слоев, объединение слоев, количество и форма фильтра...)\n",
    "   - Настройка гиперпараметров сети (скорость обучения, оптимизатор)\n",
    "\n",
    "- Извлечение признаков:\n",
    "   - Используйте STFT: необработанная спектрограмма может предоставить CNN больше информации для изучения корреляции между частотой и временем, чем MFCC.\n",
    "   - Используйте мел-спектограмму: Мел-спектограмма может предоставить CNN больше информации для изучения корреляции между частотой и временем, чем MFCC."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "UrbanSound8k_machine_learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
